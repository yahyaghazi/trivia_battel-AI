# üß† Trivia Battle AI

Un comparateur de mod√®les d'IA local pour √©valuer leurs performances sur des questions de culture g√©n√©rale. Testez et comparez vos mod√®les Ollama sur des milliers de questions trivia !

## üéØ Fonctionnalit√©s

- **Comparaison multi-mod√®les** : Teste plusieurs mod√®les Ollama simultan√©ment
- **Tests parall√®les** : Acc√©l√®re les comparaisons gr√¢ce au multi-threading
- **Questions √©quitables** : M√™me ordre de r√©ponses pour tous les mod√®les
- **Extraction robuste** : G√®re diff√©rents formats de r√©ponses (A, B, C, D)
- **Rapport d√©taill√©** : Classement, statistiques par difficult√©, analyse des biais
- **Mode debug** : Affichage d√©taill√© pour diagnostiquer les probl√®mes
- **T√©l√©chargeur int√©gr√©** : R√©cup√®re automatiquement une base de 4600+ questions

## üìã Pr√©requis

- **Python 3.7+**
- **Ollama** install√© et en cours d'ex√©cution
- **Mod√®les Ollama** t√©l√©charg√©s (ex: qwen, llama, gemma, mistral)

## üöÄ Installation

1. **Cloner le projet**
```bash
git clone https://github.com/votre-username/trivia_battle-AI.git
cd trivia_battle-AI
```

2. **Installer les d√©pendances**
```bash
pip install -r requirements.txt
```

3. **V√©rifier qu'Ollama fonctionne**
```bash
ollama list
```

## üíæ Obtenir les questions trivia

### Option 1 : T√©l√©charger automatiquement
```bash
python trivia_game.py
```
- T√©l√©charge ~4600 questions depuis Open Trivia Database
- Strat√©gie adaptative (rapide puis prudent)
- G√©n√®re un fichier CSV pr√™t √† utiliser

### Option 2 : Utiliser un fichier existant
Placez votre fichier CSV avec les colonnes :
- `question`
- `correct_answer` 
- `incorrect_answer_1`
- `incorrect_answer_2`
- `incorrect_answer_3`

## üéÆ Utilisation

### Lancement principal
```bash
python AI_reponse.py
```

### Interface interactive

1. **S√©lection des mod√®les**
   - `0` : Tous les mod√®les
   - `1,2,3` : Mod√®les sp√©cifiques

2. **Configuration**
   - Nombre de questions par mod√®le
   - D√©lai entre questions 
   - Mode de test (parall√®le/verbose)

3. **Ex√©cution**
   - Tests en parall√®le pour rapidit√©
   - Suivi en temps r√©el
   - Sauvegarde automatique des r√©sultats

## üìä Exemples de r√©sultats

```
üèÜ CLASSEMENT FINAL DES MOD√àLES
======================================
RANG MOD√àLE              PR√âCISION    SCORE    TAILLE
ü•á   mistral:latest      78.5%        157/200  3.8GB
ü•à   llama3.2:latest     72.0%        144/200  1.9GB  
ü•â   gemma3:latest       68.5%        137/200  3.1GB
4.   codellama:7b        45.2%        90/200   3.6GB
5.   qwen3:0.6b          20.0%        40/200   0.5GB
```

## üîß Modes de test

### Mode Parall√®le (Recommand√©)
- Teste plusieurs mod√®les simultan√©ment
- Plus rapide
- Suivi global des progr√®s
- Id√©al pour comparer 3-5 mod√®les

### Mode Verbose (Debug)
- Teste un mod√®le √† la fois
- Affiche chaque question et r√©ponse
- Parfait pour diagnostiquer les probl√®mes
- Voir pourquoi un mod√®le √©choue

## üìÅ Fichiers g√©n√©r√©s

```
model_comparison_summary_YYYYMMDD_HHMMSS.csv   # R√©sultats par mod√®le
model_comparison_detailed_YYYYMMDD_HHMMSS.csv  # Toutes les r√©ponses
trivia_complete_4619q_YYYYMMDD_HHMMSS.csv       # Base de questions
trivia_complete_4619q_YYYYMMDD_HHMMSS_stats.txt # Statistiques
```

## ‚öôÔ∏è Configuration avanc√©e

### Param√®tres par mod√®le
Le code optimise automatiquement :
- **Timeout** : Plus long pour les gros mod√®les
- **Tokens max** : Adapt√© √† la taille du mod√®le  
- **Temp√©rature** : 0.1 pour coh√©rence

### Extraction des r√©ponses
G√®re automatiquement :
- Balises `<think>` (Qwen, DeepSeek)
- R√©ponses "The answer is A"
- Lettres seules "B"
- Patterns multiples

## üêõ R√©solution de probl√®mes

### Mod√®le donne toujours "A"
- Mod√®le trop petit (0.6B) ou mal configur√©
- Essayez un mod√®le plus gros (3B+)

### Extraction √©choue
- Activez le mode verbose pour voir les r√©ponses brutes
- Le mod√®le ne suit peut-√™tre pas les instructions

### Ollama non accessible
```bash
# Red√©marrer Ollama
ollama serve

# V√©rifier les mod√®les
ollama list
```

### Performances lentes
- R√©duisez le nombre de questions
- Augmentez le d√©lai entre requ√™tes
- Utilisez moins de threads parall√®les

## üìà Interpr√©tation des r√©sultats

### Scores typiques
- **80-90%** : Excellent (GPT-4 niveau)
- **70-80%** : Tr√®s bon (mod√®les 7B+ r√©cents)
- **60-70%** : Bon (mod√®les 3B+ ou 7B anciens)
- **40-60%** : Moyen (connaissances limit√©es)
- **20-40%** : Faible (mod√®le trop petit)
- **<20%** : Probl√®me technique ou mod√®le inadapt√©

### Analyse par difficult√©
Le rapport d√©taille les performances sur :
- **Easy** : Questions basiques
- **Medium** : Culture g√©n√©rale standard  
- **Hard** : Connaissances sp√©cialis√©es

## ü§ù Contribution

1. Fork le projet
2. Cr√©ez une branche (`git checkout -b feature/am√©lioration`)
3. Commit (`git commit -am 'Ajoute une fonctionnalit√©'`)
4. Push (`git push origin feature/am√©lioration`)
5. Ouvrez une Pull Request

## üìù Am√©liorations possibles

- [ ] Support d'autres APIs (Anthropic, OpenAI)
- [ ] Interface web
- [ ] Graphiques de performance
- [ ] Tests par cat√©gorie
- [ ] Sauvegarde en base de donn√©es
- [ ] Mode tournoi entre mod√®les

## üìú Licence

MIT License - Voir le fichier LICENSE pour les d√©tails.

## üôè Remerciements

- [Open Trivia Database](https://opentdb.com/) pour les questions
- [Ollama](https://ollama.ai/) pour les mod√®les locaux
- Communaut√© IA open source

## üìû Support

- **Issues** : Probl√®mes ou suggestions sur GitHub
- **Discussions** : Questions g√©n√©rales dans les Discussions
- **Wiki** : Documentation d√©taill√©e (√† venir)

---

üéØ **Objectif** : Trouver le meilleur mod√®le local pour vos besoins de trivia et culture g√©n√©rale !

üí° **Astuce** : Commencez avec 50 questions sur 3-4 mod√®les pour avoir un aper√ßu rapide, puis lancez un test complet sur les plus prometteurs.